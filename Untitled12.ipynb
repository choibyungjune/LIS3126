{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Iiqa9Q7z210"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size=768,\n",
        "                 num_classes=None,  # num_classes는 클래스 내에서 정의되지 않도록 None으로 초기화\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
        "                            attention_mask=attention_mask.float().to(token_ids.device))\n",
        "        pooler = outputs[1]\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n",
        "# GbmInfer 클래스 내부 함수로 정의되어 있던 부분을 클래스 외부로 이동\n",
        "import re\n",
        "import io\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def del_bracket(s):\n",
        "    pattern = r'\\([^)]*\\)'  # ()\n",
        "    s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "    pattern = r'\\[[^)]*\\]'  # []\n",
        "    s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "    pattern = r'\\<[^)]*\\>'  # <>\n",
        "    s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "    pattern = r'\\{[^)]*\\}'  # {}\n",
        "    s = re.sub(pattern=pattern, repl='', string=s)\n",
        "\n",
        "    return s\n",
        "\n",
        "def del_special_num(s):\n",
        "    pattern = r'[^a-zA-Z가-힣]'\n",
        "    s = re.sub(pattern=pattern, repl=' ', string=s)\n",
        "\n",
        "    return s\n",
        "\n",
        "def del_unit(s):\n",
        "    units = ['mm', 'cm', 'km', 'ml', 'kg', 'g']\n",
        "    for unit in units:\n",
        "        s = s.lower()\n",
        "        s = s.replace(unit, '')\n",
        "    return s\n",
        "\n",
        "def del_whitespace(s):\n",
        "    return \" \".join(s.split())\n",
        "\n",
        "def del_stopwords(s):\n",
        "    stopwords = open(r\"/project/work/example/stopwords.txt\", 'r', encoding=\"utf-8\").read().split()\n",
        "    s_o = s.split()\n",
        "    s_f = []\n",
        "    for w in s_o:\n",
        "        if w.strip() not in stopwords:\n",
        "            s_f.append(w.strip())\n",
        "    return \" \".join(s_f)\n",
        "\n",
        "def get_inputs(tokens):\n",
        "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "    valid_length = len(tokens)\n",
        "    segment_ids = [0] * valid_length\n",
        "    attention_mask = [1] * valid_length\n",
        "\n",
        "    if valid_length < max_length:\n",
        "        pad_length = max_length - valid_length\n",
        "        tokens.extend(['[PAD]' for _ in range(pad_length)])\n",
        "        attention_mask.extend([0] * pad_length)\n",
        "        segment_ids.extend([0] * pad_length)\n",
        "\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    return torch.tensor([token_ids], dtype=torch.long), torch.tensor([valid_length], dtype=torch.long), torch.tensor(\n",
        "        [segment_ids], dtype=torch.long), torch.tensor([attention_mask], dtype=torch.long)\n",
        "\n",
        "def preprocess(text):\n",
        "    for t in text:\n",
        "        t = del_bracket(t)\n",
        "        t = del_special_num(t)\n",
        "        t = del_whitespace(t)\n",
        "        t = del_stopwords(t)\n",
        "    return text.lower()\n",
        "\n",
        "def temperature_scaled_softmax(output, temperature=1.0):\n",
        "    output = output / temperature\n",
        "    probabilities = F.softmax(output, dim=-1)\n",
        "    return probabilities\n",
        "\n",
        "# GbmInfer 클래스 내부 변수 및 함수를 클래스 외부로 이동\n",
        "class GbmInfer(dlp.Inference):\n",
        "    modelname = \"klue/bert-base\"\n",
        "    model_path = r\"/project/work/logs/20240111-modelklue/bert-base-data417791/aug1555499-class7-batch64-lr1e-05-epoch10-maxlen644_0.7754894814858444_model.pt\"\n",
        "\n",
        "    max_length = 64\n",
        "    num_classes = 7\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 클래스 인스턴스 생성\n",
        "    gbm_infer = GbmInfer()\n",
        "\n",
        "    # input 함수를 통해 사용자 입력 받음\n",
        "    text = input(\"Input the diary contents: \")\n",
        "\n",
        "    tokens = gbm_infer.tokenizer.tokenize(gbm_infer.preprocess(text))\n",
        "    if len(tokens) > gbm_infer.max_length - 2:\n",
        "        tokens = tokens[:gbm_infer.max_length - 2]\n",
        "\n",
        "    token_ids, valid_length, segment_ids, attention_mask = gbm_infer.get_inputs(tokens)\n",
        "\n",
        "    token_ids = token_ids.to(gbm_infer.device)\n",
        "    valid_length = valid_length.to(gbm_infer.device)\n",
        "    segment_ids = segment_ids.to(gbm_infer.device)\n",
        "    attention_mask = attention_mask.to(gbm_infer.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sentiment_vector = gbm_infer.model(token_ids, valid_length, segment_ids, attention_mask)\n",
        "\n",
        "    print(\"Request successful\")\n",
        "    print(sentiment_vector)\n",
        "\n",
        "    output = sentiment_vector\n",
        "    probabilities = gbm_infer.temperature_scaled_softmax(output, temperature=5.0)\n",
        "\n",
        "    emotions = ['기쁨을', '불안을', '당황을', '슬픔을', '분노를', '중립 ', '혐오를']\n",
        "\n",
        "    sorted_indices = torch.argsort(probabilities, dim=-1, descending=True)\n",
        "\n",
        "    primary_emotion_idx = None\n",
        "    secondary_emotion_idx = None\n",
        "    for idx in sorted_indices[0]:\n",
        "        if emotions[idx.item()] != '중립':\n",
        "            if primary_emotion_idx is None:\n",
        "                primary_emotion_idx = idx.item()\n",
        "            elif secondary_emotion_idx is None:\n",
        "                secondary_emotion_idx = idx.item()\n",
        "                break\n",
        "\n",
        "    primary_emotion = emotions[primary_emotion_idx]\n",
        "    primary_emotion_probability = probabilities[0][primary_emotion_idx].item()\n",
        "\n",
        "    secondary_emotion = emotions[secondary_emotion_idx]\n",
        "    secondary_emotion_probability = probabilities[0][secondary_emotion_idx].item()\n",
        "\n",
        "    print(f\"당신은 지금 {primary_emotion} 느끼고 있네요.\")\n",
        "    print(\n",
        "        f\"주 감정: {primary_emotion_probability * 100:.2f}% {primary_emotion[:-1]}, 부 감정: {secondary_emotion_probability * 100:.2f}% {secondary_emotion[:-1]}\")\n"
      ]
    }
  ]
}